{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_arr = np.loadtxt(\"wifi_db/clean_dataset.txt\")\n",
    "noisy_arr = np.loadtxt(\"wifi_db/noisy_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-59., -53., -51., ..., -79., -87.,   4.],\n",
       "       [-66., -53., -59., ..., -81., -79.,   1.],\n",
       "       [-41., -57., -63., ..., -66., -65.,   2.],\n",
       "       ...,\n",
       "       [-57., -54., -56., ..., -79., -82.,   1.],\n",
       "       [-56., -52., -50., ..., -85., -88.,   3.],\n",
       "       [-46., -54., -47., ..., -80., -73.,   3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "LABEL_COL = clean_arr.shape[1] - 1 # LABEL_COL = 7\n",
    "DATASET_SIZE = clean_arr.shape[0]\n",
    "NUM_FOLDS = 10\n",
    "labels = [1, 2, 3, 4]\n",
    "noisy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    # if node is leaf, value is used to store label\n",
    "    # if node is parent, value is used to store the split_value AND\n",
    "    # attr is used to store the attribute that is split on\n",
    "    def __init__(self, **kwargs):\n",
    "        self.label = kwargs.get('label', -1)\n",
    "        self.value = kwargs.get('value', 0)\n",
    "        self.attr = kwargs.get('attr', 0)\n",
    "        self.left = kwargs.get('left', None)\n",
    "        self.right = kwargs.get('right', None)\n",
    "    \n",
    "    def set_left(self, left):\n",
    "        self.left = left\n",
    "    \n",
    "    def set_right(self, right):\n",
    "        self.right = right\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.left == None and self.right == None\n",
    "    \n",
    "    def is_parent_of_leafs(self):\n",
    "        return self.is_leaf(self.left) and self.is_leaf(self.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_label_instances(dataset, label):\n",
    "    return np.count_nonzero(dataset[:, LABEL_COL] == label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the log operation may return nan (eg log 0), replace all nan occurences with 0\n",
    "def entropy(dataset):\n",
    "    dataset_size,_ = dataset.shape\n",
    "    res = np.zeros(len(labels))\n",
    "    for i in range(len(labels)):\n",
    "        res[i] = calc_label_instances(dataset, labels[i])\n",
    "    res = res / dataset_size\n",
    "    log_res = np.where(res > 0, np.log2(res), res)\n",
    "    res = res * log_res\n",
    "    return - np.sum(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate info gain after splitting data set, assume dataset is sorted already\n",
    "def remainder(dataset, split_index):\n",
    "    #print(\"in remainder, dataset is \", dataset);\n",
    "    dataset_size,_ = dataset.shape\n",
    "    left_info_gain = (split_index + 1) / dataset_size * entropy(dataset[: (split_index + 1)])\n",
    "    right_info_gain = (dataset_size - split_index)/dataset_size * entropy(dataset[(split_index + 1):])\n",
    "    return left_info_gain + right_info_gain\n",
    "\n",
    "\n",
    "def calc_info_gain(start_entropy, dataset, split_index):\n",
    "    return start_entropy - remainder(dataset, split_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mid(a, b):\n",
    "    return (a + b) / 2\n",
    "\n",
    "def find_split(trng_data):\n",
    "    split_attribute = -1\n",
    "    split_value = 0\n",
    "    split_index = 0\n",
    "    overall_highest_info_gain = 0\n",
    "    # start_entropy calculated at the start, saving recomputation\n",
    "    start_entropy = entropy(trng_data)\n",
    "    dataset_size, _ = trng_data.shape\n",
    "    final_sorted_dataset = trng_data\n",
    "    \n",
    "    # sort all data amongst columns\n",
    "    # loop through each attribute\n",
    "    for i in range(LABEL_COL):#\n",
    "        sorted_dataset = trng_data[trng_data[:, i].argsort()]#\n",
    "#         print('sorted dataset')\n",
    "#         print(sorted_dataset[:, [i, LABEL_COL]]);\n",
    "        current_split_value = 0\n",
    "        feature_highest_info_gain = 0\n",
    "        current_split_index = 0\n",
    "\n",
    "        for j in range(dataset_size - 1):\n",
    "            if sorted_dataset[j][LABEL_COL] == sorted_dataset[j + 1][LABEL_COL]: continue\n",
    "\n",
    "            mid = get_mid(sorted_dataset[j][i], sorted_dataset[j + 1][i])\n",
    "            info_gain = calc_info_gain(start_entropy, sorted_dataset, j)\n",
    "\n",
    "            if (not math.isnan(info_gain)) and info_gain > feature_highest_info_gain:\n",
    "                current_split_value = mid\n",
    "                feature_highest_info_gain = info_gain\n",
    "                current_split_index = j\n",
    "\n",
    "        if overall_highest_info_gain < feature_highest_info_gain:\n",
    "            overall_highest_info_gain = feature_highest_info_gain\n",
    "            split_attribute = i\n",
    "            split_value = current_split_value\n",
    "            split_index = current_split_index\n",
    "            final_sorted_dataset = sorted_dataset\n",
    "#         print(\"Feature \" + str(i) + \" info gain: \" + str(feature_highest_info_gain))\n",
    "\n",
    "#     print(\"Start Entropy: \" + str(start_entropy))\n",
    "#     print(\"Info Gain: \" + str(overall_highest_info_gain))\n",
    "        \n",
    "    return (split_attribute, split_value, split_index, final_sorted_dataset)\n",
    "\n",
    "# find_split(clean_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_learning(training_dataset, depth):\n",
    "    first_label = training_dataset[0][LABEL_COL]\n",
    "    if np.all(training_dataset[:, LABEL_COL] == first_label):\n",
    "        return (Node(label=first_label), depth)\n",
    "    else:\n",
    "        split_attr, split_value, split_index, sorted_dataset = find_split(training_dataset)\n",
    "        curr_node = Node(value=split_value, attr=split_attr)\n",
    "        left_branch, left_depth = decision_tree_learning(sorted_dataset[:split_index + 1], depth + 1)\n",
    "        right_branch, right_depth = decision_tree_learning(sorted_dataset[split_index + 1:], depth + 1)\n",
    "        curr_node.set_left(left_branch)\n",
    "        curr_node.set_right(right_branch)\n",
    "    return (curr_node, max(left_depth, right_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root, depth = decision_tree_learning(clean_arr, 0)\n",
    "# dirty_r, d = decision_tree_learning(noisy_arr, 0)\n",
    "# print using py_plot\n",
    "def print_nodes(node):\n",
    "    if node.left != None:\n",
    "        print_nodes(node.left)\n",
    "    if node.attr != None:\n",
    "        print(node.attr)\n",
    "    print(node.value)\n",
    "    if node.right != None:\n",
    "        print_nodes(node.right)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take class 1 as positive\n",
    "\n",
    "def traverse_tree(root, datapoint):\n",
    "    if(root.left is None and root.right is None):\n",
    "        return root.label\n",
    "    attr = root.attr\n",
    "    split_val = root.value\n",
    "    if(datapoint[attr] <= split_val):\n",
    "        return traverse_tree(root.left, datapoint)\n",
    "    else:\n",
    "        return traverse_tree(root.right, datapoint)\n",
    "    \n",
    "\n",
    "def evaluate(test_db, trained_tree):\n",
    "    # returns accuracy of tree\n",
    "    confusion_matrix = np.zeros(16).reshape((4, 4))\n",
    "    for i in range(len(test_db)):\n",
    "        classified_label = traverse_tree(trained_tree, test_db[i])\n",
    "        actual_label = test_db[i][LABEL_COL]\n",
    "        confusion_matrix[int(actual_label) - 1,int(classified_label) - 1] += 1\n",
    "    \n",
    "    true_positive = confusion_matrix[0,0] \n",
    "    true_negative = confusion_matrix[1,1] + confusion_matrix[2,2] + confusion_matrix[3,3]\n",
    "    all_values = len(test_db)\n",
    "    accurately_classified_data = true_positive + true_negative\n",
    "    accuracy = accurately_classified_data / all_values\n",
    "    \n",
    "    return (confusion_matrix, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9744999999999999\n[0.98997642 0.96240741 0.95880759 0.98577164]\n[0.98479867 0.97015512 0.95680806 0.98604308]\n[0.98738076 0.96626574 0.95780678 0.98590734]\n[[49.5  0.   0.1  0.4]\n [ 0.  48.1  1.9  0. ]\n [ 0.2  1.5 48.   0.3]\n [ 0.6  0.   0.1 49.3]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_recall(matrix, i):\n",
    "    row = matrix[i]\n",
    "    return matrix[i][i] / sum(row)\n",
    "\n",
    "def calculate_precision(matrix, i):\n",
    "    col = matrix[:, i]\n",
    "    return matrix[i][i] / sum(col)\n",
    "\n",
    "def cross_validation(data):\n",
    "    recall_per_class = np.zeros(4)\n",
    "    precision_per_class = np.zeros(4)\n",
    "    f1_per_class = np.zeros(4)\n",
    "    sum_accuracy = 0\n",
    "    sum_confusion_matrix = np.zeros(16).reshape((4, 4))\n",
    "    np.random.shuffle(data)\n",
    "    for i in range(NUM_FOLDS):\n",
    "        begin = i * int(len(data) / NUM_FOLDS)\n",
    "        end = (i + 1) * int(len(data) / NUM_FOLDS)\n",
    "        test_data = data[begin: end]\n",
    "        trng_data = np.delete(data, slice(begin, end), axis=0)\n",
    "        \n",
    "        root, depth = decision_tree_learning(trng_data, 0)\n",
    "        confusion_matrix, accuracy = evaluate(test_data, root)\n",
    "        sum_accuracy += accuracy\n",
    "        sum_confusion_matrix += confusion_matrix\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            recall_per_class[i] += calculate_recall(confusion_matrix, i)\n",
    "            precision_per_class[i] += calculate_precision(confusion_matrix, i)\n",
    "        \n",
    "    average_recall_per_class = recall_per_class / NUM_FOLDS\n",
    "    average_precision_per_class = precision_per_class / NUM_FOLDS\n",
    "    f1_per_class = 2 * average_recall_per_class * average_precision_per_class / (average_recall_per_class + average_precision_per_class)\n",
    "    avg_accuracy = sum_accuracy / NUM_FOLDS\n",
    "    avg_confusion_matrix = sum_confusion_matrix / NUM_FOLDS\n",
    "    return (avg_accuracy, average_recall_per_class, average_precision_per_class, f1_per_class, avg_confusion_matrix)\n",
    "\n",
    "acc, recall, precision, f1, conf_matrix = cross_validation(clean_arr)\n",
    "print(acc)\n",
    "print(recall)\n",
    "print(precision)\n",
    "print(f1)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning\n",
    "# 1. Go through each internal node that are connected only to leaf nodes.\n",
    "# 2. Turn each into a leaf node (with majority class label)\n",
    "# 3. Evaluate pruned tree on validation set. Prune if accuracy higher than unpruned.\n",
    "\n",
    "def prune(root):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}